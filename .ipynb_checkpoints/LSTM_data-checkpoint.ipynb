{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdbb29cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521a1442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先遍历读入每个样本，结构为[帧数*特征数]的DataFrame\n",
    "# 然后把每个样本装入feature中，结构为list；同时对应记录下目标值，装入label中\n",
    "# 统一装入list之后，随机划分训练集/测试集\n",
    "# 生成的文件存储在data_lstm当中\n",
    "\n",
    "feature,label = [],[]\n",
    "#疲劳等级为[0,3)\n",
    "for labelIndex in range(0,3):\n",
    "    y = labelIndex\n",
    "    #每个等级的样本数量#############\n",
    "    sampleNum = 0\n",
    "    if labelIndex == 0:\n",
    "        sampleNum = 215\n",
    "    elif labelIndex == 1:\n",
    "        sampleNum = 100\n",
    "    else:\n",
    "        sampleNum = 144\n",
    "\n",
    "    #遍历读取样本数据\n",
    "    for sampleIndex in range(1,sampleNum + 1):\n",
    "        x = pd.read_csv(\"./data_new/level_\"+ str(labelIndex) + \"/\" + str(labelIndex)+\"_\" + str(sampleIndex) + \".csv\")\n",
    "        # 暂时做了补帧处理，后续删除，最多有85帧###########################################\n",
    "        frameNum = x.shape[0]# 帧数\n",
    "        if frameNum < 85:\n",
    "            if frameNum >0: # 为了跳过空数据\n",
    "                # 暂时选取了最后一行的数据填补\n",
    "                head_amplitude = x.iloc[x.shape[0]-1].at['head_amplitude']\n",
    "                eyes_ratio = x.iloc[x.shape[0]-1].at['eyes_ratio']\n",
    "                K = x.iloc[x.shape[0]-1].at['K']\n",
    "                for frame in range(frameNum + 1,86):\n",
    "                    x.loc[len(x.index)] = [frame,head_amplitude,eyes_ratio,K]\n",
    "            else:\n",
    "                for frame in range(frameNum + 1,86):\n",
    "                    x.loc[len(x.index)] = [frame,0,0,0]\n",
    "        \n",
    "        # 把每个样本装入feature中\n",
    "        #feature.append(x.values.tolist())\n",
    "        x.drop(x.columns[[0]],axis = 1,inplace=True)\n",
    "#         print(x)\n",
    "        feature.append(x)\n",
    "        label.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28e334f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# 现在划分训练集：验证集：测试集 = 8：1：1\n",
    "train_feature,train_label,valid_feature,valid_label,test_feature,test_label = [],[],[],[],[],[]\n",
    "for index in range (0,459):\n",
    "    sampleFeature = feature[index]\n",
    "    sampleLabel = label[index]\n",
    "    # 生成第一个随机数\n",
    "    randNum = random.random()\n",
    "    # 如果生成的随机数小于0.8，则作为训练集；剩余的作为测试集\n",
    "    if randNum < 0.8:\n",
    "        train_feature.append(sampleFeature.values.tolist())\n",
    "        train_label.append(sampleLabel)\n",
    "    elif randNum >=0.9:\n",
    "        test_feature.append(sampleFeature.values.tolist())\n",
    "        test_label.append(sampleLabel)\n",
    "    else:\n",
    "        valid_feature.append(sampleFeature.values.tolist())\n",
    "        valid_label.append(sampleLabel)\n",
    "# print(len(train_feature))\n",
    "# print(len(train_label))\n",
    "# print(len(test_feature))\n",
    "# print(len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c535b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.core.frame import DataFrame\n",
    "\n",
    "# feature_df = DataFrame(feature)\n",
    "# feature_file_name=\"data_lstm/feature.csv\";\n",
    "# feature_df.to_csv(feature_file_name,index=False,sep=',')\n",
    "\n",
    "# label_df = DataFrame(label)\n",
    "# label_file_name=\"data_lstm/label.csv\";\n",
    "# label_df.to_csv(label_file_name,index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f6a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义常量\n",
    "INPUT_SIZE = 4  # 定义输入的特征数\n",
    "HIDDEN_SIZE = 5    # 定义一个LSTM单元有多少个神经元???????\n",
    "BATCH_SIZE = 5   # batch??????????\n",
    "EPOCH = 10    # 学习次数???????????\n",
    "LR = 0.001   # 学习率??????????????\n",
    "TIME_STEP = 28   # 步长，一般用不上，写出来就是给自己看的??????????\n",
    "DROP_RATE = 0.2    #  drop out概率?????????????\n",
    "LAYERS = 2         # 有多少隐层，一个隐层一般放一个LSTM单元????????????\n",
    "MODEL = 'LSTM'     # 模型名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95881b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一些常用函数\n",
    "# 保存日志\n",
    "# fname是要保存的位置，s是要保存的内容\n",
    "def log(fname, s):\n",
    "    f = open(fname, 'a')\n",
    "    f.write(str(datetime.now()) + ': ' + s + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82a8391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义LSTM的结构\n",
    "class lstm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lstm, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = INPUT_SIZE, \n",
    "            hidden_size = HIDDEN_SIZE, \n",
    "            num_layers = LAYERS,\n",
    "            dropout = DROP_RATE,\n",
    "            batch_first = True    # 如果为True，输入输出数据格式是(batch, seq_len, feature)\n",
    "                                  # 为False，输入输出数据格式是(seq_len, batch, feature)，\n",
    "        )\n",
    "        self.hidden_out = nn.Linear(HIDDEN_SIZE, 10)  # 最后一个时序的输出接一个全连接层\n",
    "        self.h_s = None\n",
    "        self.h_c = None\n",
    "        \n",
    "    def forward(self, x):    # x是输入数据集\n",
    "        r_out, (h_s, h_c) = self.rnn(x)   # 如果不导入h_s和h_c，默认每次都进行0初始化\n",
    "                                          #  h_s和h_c表示每一个隐层的上一时间点输出值和输入细胞状态\n",
    "                                          # h_s和h_c的格式均是(num_layers * num_directions, batch, HIDDEN_SIZE)\n",
    "                                          # 如果是双向LSTM，num_directions是2，单向是1\n",
    "        output = self.hidden_out(r_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8080aa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23ec6fd4f10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 设置随机种子\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b664bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_dataset():\n",
    "    def __init__(self,data_feature,data_label):\n",
    "        super().__init__()\n",
    "        # 读取的样本装入self.src,目标值装入self.trg\n",
    "        self.src = torch.tensor(data_feature)\n",
    "        self.trg = torch.tensor(data_label)\n",
    "#         print(self.src.size())\n",
    "#         print(self.trg.size())\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.trg[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3807eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = My_dataset(train_feature,train_label)\n",
    "valid_data = My_dataset(valid_feature,valid_label)\n",
    "test_data = My_dataset(test_feature,test_label)\n",
    "\n",
    "# # i_batch的多少根据batch size和def __len__(self)返回的长度确定\n",
    "# # batch_data返回的值根据def __getitem__(self, index)来确定\n",
    "# # 对训练集：\n",
    "# for i_batch, batch_data in enumerate(train_data):\n",
    "#     print(i_batch)  # 打印batch编号\n",
    "#     print(batch_data[0])  # 打印该batch里面src\n",
    "#     print(batch_data[1])  # 打印该batch里面trg\n",
    "# # 对测试集：\n",
    "# for i_batch, (src, trg) in enumerate(test_data):\n",
    "#     print(i_batch)  # 打印batch编号\n",
    "#     print(src)  # 打印该batch里面src的尺寸\n",
    "#     print(trg)  # 打印该batch里面trg的尺寸    \n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE,pin_memory=True, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE,pin_memory=True, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE,pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3964dc2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 4, got 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1116/647917317.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mb_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mb_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# CrossEntropy的target是longtensor，且要是1-D，不是one hot编码形式\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_x\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# rnn output    # prediction (4, 72, 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;31m#         h_s = h_s.data        # repack the hidden state, break the connection from last iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#         h_c = h_c.data        # repack the hidden state, break the connection from last iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lubixing\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1116/2650621158.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# x是输入数据集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mr_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_c\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# 如果不导入h_s和h_c，默认每次都进行0初始化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                                           \u001b[1;31m#  h_s和h_c表示每一个隐层的上一时间点输出值和输入细胞状态\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                                           \u001b[1;31m# h_s和h_c的格式均是(num_layers * num_directions, batch, HIDDEN_SIZE)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lubixing\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lubixing\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    687\u001b[0m             \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[1;32mc:\\users\\lubixing\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    630\u001b[0m                            \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m                            ):\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[0;32m    634\u001b[0m                                'Expected hidden[0] size {}, got {}')\n",
      "\u001b[1;32mc:\\users\\lubixing\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    205\u001b[0m             raise RuntimeError(\n\u001b[0;32m    206\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[1;32m--> 207\u001b[1;33m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_expected_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 4, got 3"
     ]
    }
   ],
   "source": [
    "rnn = lstm().to(device)    # 使用GPU或CPU\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss() # 分类问题\n",
    "\n",
    "# 定义学习率衰减点，训练到50%和75%时学习率缩小为原来的1/10\n",
    "mult_step_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                             milestones=[EPOCH//2, EPOCH//4*3], gamma=0.1)\n",
    "\n",
    "# 训练+验证\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "min_valid_loss = np.inf\n",
    "for i in range(EPOCH):\n",
    "    total_train_loss = []    \n",
    "    rnn.train()   # 进入训练模式\n",
    "    for step, (b_x, b_y) in enumerate(train_loader):\n",
    "#         lr = set_lr(optimizer, i, EPOCH, LR)\n",
    "        b_x = b_x.type(torch.FloatTensor).to(device)   # \n",
    "        b_y = b_y.type(torch.long).to(device)   # CrossEntropy的target是longtensor，且要是1-D，不是one hot编码形式\n",
    "        prediction = rnn(b_x) # rnn output    # prediction (4, 72, 2)\n",
    "#         h_s = h_s.data        # repack the hidden state, break the connection from last iteration\n",
    "#         h_c = h_c.data        # repack the hidden state, break the connection from last iteration\n",
    "        loss = loss_func(prediction[:, -1, :], b_y.view(b_y.size()[0]))         # 计算损失，target要转1-D，注意b_y不是one hot编码形式\n",
    "        optimizer.zero_grad()                   # clear gradients for this training step\n",
    "        loss.backward()                         # backpropagation, compute gradients\n",
    "        optimizer.step()                        # apply gradients\n",
    "        total_train_loss .append(loss.item())\n",
    "    train_loss.append(np.mean(total_train_loss )) # 存入平均交叉熵\n",
    "  \n",
    "    \n",
    "    total_valid_loss = [] \n",
    "    rnn.eval()\n",
    "    for step, (b_x, b_y) in enumerate(valid_loader):\n",
    "        b_x = b_x.type(torch.FloatTensor).to(device) \n",
    "        b_y = b_y.type(torch.long).to(device) \n",
    "        with torch.no_grad():\n",
    "            prediction = rnn(b_x) # rnn output\n",
    "#         h_s = h_s.data        # repack the hidden state, break the connection from last iteration\n",
    "#         h_c = h_c.data        # repack the hidden state, break the connection from last iteration\n",
    "        loss = loss_func(prediction[:, -1, :], b_y.view(b_y.size()[0]))         # calculate loss        \n",
    "        total_valid_loss.append(loss.item())        \n",
    "    valid_loss.append(np.mean(total_valid_loss))\n",
    "    \n",
    "    if (valid_loss[-1] < min_valid_loss):      \n",
    "        torch.save({'epoch': i, 'model': rnn, 'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss},'./LSTM.model') # 保存字典对象，里面'model'的value是模型\n",
    "#         torch.save(optimizer, './LSTM.optim')     # 保存优化器      \n",
    "        min_valid_loss = valid_loss[-1]\n",
    "        \n",
    "    # 编写日志\n",
    "    log_string = ('iter: [{:d}/{:d}], train_loss: {:0.6f}, valid_loss: {:0.6f}, '\n",
    "                      'best_valid_loss: {:0.6f}, lr: {:0.7f}').format((i + 1), EPOCH,\n",
    "                                                                      train_loss[-1],\n",
    "                                                                      valid_loss[-1],\n",
    "                                                                      min_valid_loss,\n",
    "                                                                      optimizer.param_groups[0]['lr'])\n",
    "    mult_step_scheduler.step()  # 学习率更新\n",
    "    # 服务器一般用的世界时，需要加8个小时，可以视情况把加8小时去掉\n",
    "    print(str(datetime.datetime.now()+datetime.timedelta(hours=8)) + ': ')\n",
    "    print(log_string)    # 打印日志\n",
    "    log('./LSTM.log', log_string)   # 保存日志\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e555012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "best_model = torch.load('./LSTM.model').get('model').cuda()\n",
    "best_model.eval()\n",
    "final_predict = []\n",
    "ground_truth = []\n",
    "\n",
    "for step, (b_x, b_y) in enumerate(test_loader):\n",
    "    b_x = b_x.type(torch.FloatTensor).to(device) \n",
    "    b_y = b_y.type(torch.long).to(device) \n",
    "    with torch.no_grad():\n",
    "        prediction = best_model(b_x) # rnn output\n",
    "#     h_s = h_s.data        # repack the hidden state, break the connection from last iteration\n",
    "#     h_c = h_c.data        # repack the hidden state, break the connection from last iteration\n",
    "    \n",
    "    loss = loss_func(prediction[:, -1, :], b_y.view(b_y.size()[0]))          # calculate loss\n",
    "    \n",
    "    ground_truth = ground_truth + b_y.view(b_y.size()[0]).cpu().numpy().tolist()\n",
    "    final_predict = final_predict + torch.max(prediction[:, -1, :], 1)[1].cpu().data.numpy().tolist()\n",
    "\n",
    "ground_truth = np.asarray(ground_truth)\n",
    "final_predict = np.asarray(final_predict)\n",
    "\n",
    "accuracy = float((ground_truth == final_predict).astype(int).sum()) / float(final_predict.size)\n",
    "print(accuracy)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf6fee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
