{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdbb29cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import csv\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime  # 用于计算时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f391e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_dataset():\n",
    "    def __init__(self,data_feature,data_label):\n",
    "        super().__init__()\n",
    "        # 读取的样本装入self.src,目标值装入self.trg\n",
    "        self.src = torch.tensor(data_feature)\n",
    "        self.trg = torch.tensor(data_label)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.trg[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "521a1442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先遍历读入每个样本，结构为[帧数*特征数]的DataFrame\n",
    "# 然后把每个样本装入feature中，结构为list；同时对应记录下目标值，装入label中\n",
    "# 统一装入list之后，随机划分训练集/测试集\n",
    "\n",
    "feature,label = [],[]\n",
    "#疲劳等级为[0,3)\n",
    "for labelIndex in range(0,3):\n",
    "    y = labelIndex\n",
    "    sampleNum = 0\n",
    "    if labelIndex == 0:\n",
    "        sampleNum = 199\n",
    "    elif labelIndex == 1:\n",
    "        sampleNum = 90\n",
    "    else:\n",
    "        sampleNum = 132\n",
    "\n",
    "    #遍历读取样本数据\n",
    "    for sampleIndex in range(1,sampleNum + 1):\n",
    "        x = pd.read_csv(\"../data_final/level_\"+ str(labelIndex) + \"/\" + str(labelIndex)+\"_\" + str(sampleIndex) + \".csv\")        \n",
    "        x.drop(x.columns[[0]],axis = 1,inplace=True)\n",
    "        x.drop(x.columns[[0]],axis = 1,inplace=True)\n",
    "        # 把每个样本装入feature中\n",
    "        feature.append(x)\n",
    "        label.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e7c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 现在划分训练集：验证集：测试集 = 8：1：1\n",
    "train_feature,train_label,valid_feature,valid_label,test_feature,test_label = [],[],[],[],[],[]\n",
    "# 特征工程：标准化\n",
    "standard_transfer = StandardScaler()\n",
    "\n",
    "for index in range (0,421):\n",
    "    sampleFeature = feature[index]\n",
    "    sampleLabel = label[index]\n",
    "    # 生成第一个随机数\n",
    "    randNum = random.random()\n",
    "    # 如果生成的随机数小于0.8，则作为训练集；剩余的作为测试集\n",
    "    if randNum < 0.8:\n",
    "        sampleFeature = standard_transfer.fit_transform(sampleFeature)\n",
    "        train_feature.append(sampleFeature.tolist())\n",
    "        train_label.append(sampleLabel)\n",
    "    elif randNum >=0.9:\n",
    "        sampleFeature = standard_transfer.transform(sampleFeature)\n",
    "        test_feature.append(sampleFeature.tolist())\n",
    "        test_label.append(sampleLabel)\n",
    "    else:\n",
    "        sampleFeature = standard_transfer.fit_transform(sampleFeature)\n",
    "        valid_feature.append(sampleFeature.tolist())\n",
    "        valid_label.append(sampleLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca63e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义常量\n",
    "INPUT_SIZE = 2  # 定义输入的特征数\n",
    "HIDDEN_SIZE = 32    # 定义一个LSTM单元有多少个神经元\n",
    "BATCH_SIZE = 32   # batch\n",
    "EPOCH = 15    # 学习次数\n",
    "LR = 0.001   # 学习率\n",
    "TIME_STEP = 30   # 步长\n",
    "DROP_RATE = 0.1    #  drop out概率\n",
    "LAYERS = 2         # 有多少隐层，一个隐层一般放一个LSTM单元\n",
    "MODEL = 'LSTM'     # 模型名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ea3f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = My_dataset(train_feature,train_label)\n",
    "valid_data = My_dataset(valid_feature,valid_label)\n",
    "test_data = My_dataset(test_feature,test_label)\n",
    "\n",
    "# # i_batch的多少根据batch size和def __len__(self)返回的长度确定\n",
    "# # batch_data返回的值根据def __getitem__(self, index)来确定\n",
    "# # 对训练集：\n",
    "# for i_batch, batch_data in enumerate(train_data):\n",
    "#     print(i_batch)  # 打印batch编号\n",
    "#     print(batch_data[0])  # 打印该batch里面src\n",
    "#     print(batch_data[1])  # 打印该batch里面trg\n",
    "# # 对测试集：\n",
    "# for i_batch, (src, trg) in enumerate(test_data):\n",
    "#     print(i_batch)  # 打印batch编号\n",
    "#     print(src)  # 打印该batch里面src的尺寸\n",
    "#     print(trg)  # 打印该batch里面trg的尺寸    \n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE,pin_memory=True, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE,pin_memory=True, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE,pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb711a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义LSTM的结构\n",
    "class lstm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lstm, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = INPUT_SIZE, \n",
    "            hidden_size = HIDDEN_SIZE, \n",
    "            num_layers = LAYERS,\n",
    "            dropout = DROP_RATE,\n",
    "            batch_first = True    # 如果为True，输入输出数据格式是(batch, seq_len, feature)\n",
    "                                  # 为False，输入输出数据格式是(seq_len, batch, feature)，\n",
    "        )\n",
    "        self.hidden_out = nn.Linear(HIDDEN_SIZE, 15)  # 最后一个时序的输出接一个全连接层\n",
    "        self.h_s = None\n",
    "        self.h_c = None\n",
    "        \n",
    "    def forward(self, x):    # x是输入数据集\n",
    "        r_out, (h_s, h_c) = self.rnn(x)   # 如果不导入h_s和h_c，默认每次都进行0初始化\n",
    "                                          #  h_s和h_c表示每一个隐层的上一时间点输出值和输入细胞状态\n",
    "                                          # h_s和h_c的格式均是(num_layers * num_directions, batch, HIDDEN_SIZE)\n",
    "                                          # 如果是双向LSTM，num_directions是2，单向是1\n",
    "        output = self.hidden_out(r_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cfe4cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2d38ceb7f30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "# 设置随机种子\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3964dc2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 2, got 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17944/1430085178.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mb_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mb_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# CrossEntropy的target是longtensor，且要是1-D，不是one hot编码形式\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_x\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# rnn output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;31m# 计算损失，target要转1-D，注意b_y不是one hot编码形式\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                   \u001b[1;31m# clear gradients for this training step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lubixing\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17944/3307966228.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# x是输入数据集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mr_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_c\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# 如果不导入h_s和h_c，默认每次都进行0初始化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                                           \u001b[1;31m#  h_s和h_c表示每一个隐层的上一时间点输出值和输入细胞状态\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                                           \u001b[1;31m# h_s和h_c的格式均是(num_layers * num_directions, batch, HIDDEN_SIZE)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lubixing\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lubixing\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    687\u001b[0m             \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[1;32mc:\\users\\lubixing\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    630\u001b[0m                            \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m                            ):\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[0;32m    634\u001b[0m                                'Expected hidden[0] size {}, got {}')\n",
      "\u001b[1;32mc:\\users\\lubixing\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    205\u001b[0m             raise RuntimeError(\n\u001b[0;32m    206\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[1;32m--> 207\u001b[1;33m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_expected_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 2, got 3"
     ]
    }
   ],
   "source": [
    "rnn = lstm().to(device)    # 使用GPU或CPU\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss() # 分类问题\n",
    "\n",
    "# 定义学习率衰减点，训练到50%和75%时学习率缩小为原来的1/10\n",
    "mult_step_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                             milestones=[EPOCH//2, EPOCH//4*3], gamma=0.1)\n",
    "\n",
    "# 训练+验证\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_loss_list,valid_loss_list,index=[],[],[]\n",
    "min_valid_loss = np.inf\n",
    "for i in range(EPOCH):\n",
    "    total_train_loss = []    \n",
    "    rnn.train()   # 进入训练模式\n",
    "    for step, (b_x, b_y) in enumerate(train_loader):\n",
    "#         lr = set_lr(optimizer, i, EPOCH, LR)\n",
    "        b_x = b_x.type(torch.FloatTensor).to(device)\n",
    "        b_y = b_y.type(torch.long).to(device)   # CrossEntropy的target是longtensor，且要是1-D，不是one hot编码形式\n",
    "        prediction = rnn(b_x) # rnn output\n",
    "        loss = loss_func(prediction[:, -1, :], b_y.view(b_y.size()[0]))         # 计算损失，target要转1-D，注意b_y不是one hot编码形式        \n",
    "        optimizer.zero_grad()                   # clear gradients for this training step\n",
    "        loss.backward()                         # backpropagation, compute gradients\n",
    "        optimizer.step()                        # apply gradients\n",
    "        total_train_loss .append(loss.item())\n",
    "    train_loss.append(np.mean(total_train_loss )) # 存入平均交叉熵\n",
    "    \n",
    "    total_valid_loss = [] \n",
    "    rnn.eval()\n",
    "    for step, (b_x, b_y) in enumerate(valid_loader):\n",
    "        b_x = b_x.type(torch.FloatTensor).to(device) \n",
    "        b_y = b_y.type(torch.long).to(device) \n",
    "        with torch.no_grad():\n",
    "            prediction = rnn(b_x) # rnn output\n",
    "        loss = loss_func(prediction[:, -1, :], b_y.view(b_y.size()[0]))         # calculate loss        \n",
    "        total_valid_loss.append(loss.item())        \n",
    "    valid_loss.append(np.mean(total_valid_loss))\n",
    "    \n",
    "    if (valid_loss[-1] < min_valid_loss):      \n",
    "        torch.save({'epoch': i, 'model': rnn, 'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss},'./LSTM.model') # 保存字典对象，里面'model'的value是模型  \n",
    "        min_valid_loss = valid_loss[-1]\n",
    "    temp = pd.read_csv('./loss.csv')\n",
    "    index.append(i+temp.shape[0])\n",
    "    train_loss_list.append(train_loss[-1])\n",
    "    valid_loss_list.append(valid_loss[-1])\n",
    "    mult_step_scheduler.step()  # 学习率更新\n",
    "\n",
    "data = {\n",
    "    \"index\":index,\n",
    "    \"train_loss\": train_loss_list,\n",
    "    \"valid_loss\": valid_loss_list\n",
    "}\n",
    "loss = pd.DataFrame(data)\n",
    "#loss.to_csv('./loss.csv')\n",
    "\n",
    "\n",
    "with open('./loss.csv',mode='a',newline='',encoding='utf8') as cfa:\n",
    "    wf = csv.writer(cfa)\n",
    "    for i in range(0,15):\n",
    "        wf.writerow(loss.loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e555012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_model = torch.load('./LSTM.model').get('model')\n",
    "best_model.eval()\n",
    "final_predict = []\n",
    "ground_truth = []\n",
    "\n",
    "for step, (b_x, b_y) in enumerate(test_loader):\n",
    "    b_x = b_x.type(torch.FloatTensor).to(device) \n",
    "    b_y = b_y.type(torch.long).to(device) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = best_model(b_x) # rnn output\n",
    "    \n",
    "    loss = loss_func(prediction[:, -1, :], b_y.view(b_y.size()[0]))          # calculate loss\n",
    "    \n",
    "    ground_truth = ground_truth + b_y.view(b_y.size()[0]).cpu().numpy().tolist()\n",
    "    final_predict = final_predict + torch.max(prediction[:, -1, :], 1)[1].cpu().data.numpy().tolist()\n",
    "\n",
    "ground_truth = np.asarray(ground_truth)\n",
    "final_predict = np.asarray(final_predict)\n",
    "\n",
    "accuracy = float((ground_truth == final_predict).astype(int).sum()) / float(final_predict.size)\n",
    "\n",
    "#生成混淆矩阵\n",
    "sns.set()\n",
    "f,ax = plt.subplots()\n",
    "y_true = ground_truth\n",
    "y_pred = final_predict\n",
    "C2 = confusion_matrix(y_true,y_pred,labels=[0,1,2])\n",
    "print(C2)\n",
    "sns.heatmap(C2,annot=True,ax=ax) #画热力图\n",
    "# ax.set_titile('confusion matrix') #标题\n",
    "# ax.set_xlabel('predict') #x 轴\n",
    "# ax.set_ylabel('true') #y 轴\n",
    "\n",
    "\n",
    "# 准确度\n",
    "print(\"accuracy:\",accuracy)\n",
    "# accuracy_list = []\n",
    "# accuracy_list.append(accuracy)\n",
    "# data = {\n",
    "#     \"index\":pd.read_csv('./accuracy.csv').shape[0],\n",
    "#     \"accuracy\":accuracy_list\n",
    "# }\n",
    "# accuracy_pd = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf6fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = pd.read_csv(\"./loss.csv\")\n",
    "loss.drop(loss.columns[[0]],axis = 1,inplace=True)\n",
    "pd.DataFrame(loss)\n",
    "train_loss,valid_loss=[],[] \n",
    "train_loss_list,valid_loss_list=[],[]\n",
    "for i in range(0,loss.shape[0],15):\n",
    "    for j in range(0,15):\n",
    "        train_loss.append(loss.iloc[i+j].at['train_loss'])\n",
    "        valid_loss.append(loss.iloc[i+j].at['valid_loss'])\n",
    "        train_loss_list.append(train_loss)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "    plt.plot(train_loss, color='blue', label='train_loss')\n",
    "    plt.plot(valid_loss, color='orange', label='valid_loss')\n",
    "    train_loss=[]\n",
    "    valid_loss=[]\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd5f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a8a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
